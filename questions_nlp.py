# -*- coding: utf-8 -*-
"""Questions NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10naFj_HXKlOw34d3tf5ELAVft8cReCBC
"""

import pandas as pd
df = pd.read_excel('datas.xls')
df = df.drop(columns=['no'])
df

category = pd.get_dummies(df.category0)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='category0')
df_baru

tanya = df_baru['questions'].values
label = df_baru[['ENTITY', 'HUMAN', 'LOCATION']].values

print(type(tanya))
print(type(label))

from sklearn.model_selection import train_test_split
tanya_latih, tanya_test, label_latih, label_test = train_test_split(tanya, label, test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(oov_token='<oov>')
tokenizer.fit_on_texts(tanya_latih) 

 
sekuens_latih = tokenizer.texts_to_sequences(tanya_latih)
sekuens_test = tokenizer.texts_to_sequences(tanya_test)
 
padded_latih = pad_sequences(sekuens_latih, 
                                      padding='post',
                                      maxlen=10,
                                      truncating='post') 
padded_test = pad_sequences(sekuens_test, 
                                      padding='post',
                                      maxlen=10,
                                      truncating='post')

print("ini sekuens latih =",(sekuens_latih))
print("ini sekuens test =",(sekuens_test))
print("ini label latih =",(label_latih))
print("ini label test =",(label_test))

#Download Pre-Trained word Embedding GloVe : https://nlp.stanford.edu/projects/glove/
!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip /content/glove.6B.zip

import numpy as np
embeddings_index = {};
with open('/content/glove.6B.100d.txt') as f:
    for line in f:
        values = line.split();
        word = values[0];
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs;

embeddings_matrix = np.zeros((len(tokenizer.word_index)+1, 100))
for word, i in tokenizer.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embeddings_matrix[i] = embedding_vector

import tensorflow as tf
from tensorflow.keras import regularizers
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(tokenizer.word_index)+1, 100, input_length=15, weights=[embeddings_matrix], trainable=False),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences='true')),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(512, kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.001),
    bias_regularizer=regularizers.l2(0.0001),
    activity_regularizer=regularizers.l2(0.00001), activation='relu'),
    tf.keras.layers.Dense(512),
    tf.keras.layers.Dense(3, activation='softmax')
])
model.summary()

ACCURACY_THRESHOLD = 0.90

class myCallback(tf.keras.callbacks.Callback): 
    def on_epoch_end(self, epoch, logs={}): 
      if(logs.get('val_accuracy') >= ACCURACY_THRESHOLD):   
        print("\nVal_Akurasi Telah mencapai %2.2f%% " %(ACCURACY_THRESHOLD*100))
        self.model.stop_training = True

callbacks = myCallback()

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

num_epochs = 30
history = model.fit(padded_latih, label_latih, epochs=num_epochs,
                    validation_data=(padded_test, label_test), verbose=2,
                    callbacks=[callbacks])

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 8))
plt.grid()
plt.plot(history.history['accuracy'],color='blue')
plt.plot(history.history['val_accuracy'],color='red')
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

plt.figure(figsize=(8, 8))
plt.grid()
plt.plot(history.history['loss'],color='blue')
plt.plot(history.history['val_loss'],color='red')
plt.title('loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper right')
plt.show()

columns = []

for col in df_baru.columns:
  if col == "questions":
    pass
  else:
    columns.append(col)

tebak = ["Who made the generator ac ?"]
tebak_seq = tokenizer.texts_to_sequences(tebak)
tebak_pad = pad_sequences(tebak_seq, maxlen=10)
tebak = model.predict(tebak_pad)
klasifikasi = columns

for label in klasifikasi:
  print( label, "confidence = ", tebak[0][klasifikasi.index(label)]*100, "%" )

print("\nPrediksi jenis pertanyaan = ", klasifikasi[np.argmax(tebak)])